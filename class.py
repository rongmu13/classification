import io
import numpy as np
import pandas as pd
import streamlit as st
from typing import Dict, List

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

import matplotlib.pyplot as plt


st.set_page_config(page_title="Tabular Classifier App", layout="wide")
st.title("ğŸ§ª Tabular Classifierï¼ˆExcel ä¸€é”®åˆ†ç±»ï¼‰")

st.markdown("""
**ç”¨æ³•**  
1) ä¸Šä¼  Excelï¼ˆç¬¬ä¸€åˆ—æ˜¯**åŸå§‹æ ‡ç­¾**ï¼Œåé¢çš„åˆ—éƒ½æ˜¯ç‰¹å¾ï¼‰ã€‚  
2) é€‰æ‹©ç›®æ ‡åˆ†ç±»æ•°ï¼ˆ2/3/4/5ï¼‰ï¼ŒæŠŠâ€œåŸå§‹æ ‡ç­¾ â†’ æ–°æ ‡ç­¾(0..K-1)â€ä¸€ä¸€æ˜ å°„ã€‚  
3) é€‰æ‹©è¦ç”¨çš„ç‰¹å¾ã€åˆ†ç±»å™¨ï¼Œæ˜¯å¦ç”¨ PCAã€‚  
4) ç‚¹â€œè®­ç»ƒå¹¶è¯„ä¼°â€ã€‚  
""")

# --- Sidebar controls ---
st.sidebar.header("âš™ï¸ å‚æ•°è®¾ç½®")

test_size = st.sidebar.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)
random_state = st.sidebar.number_input("random_state", value=42, step=1)
use_pca = st.sidebar.checkbox("ä½¿ç”¨ PCAï¼ˆä»…å¯¹éæ ‘æ¨¡å‹ï¼‰", value=False)
pca_var = st.sidebar.slider("PCA ä¿ç•™æ–¹å·®æ¯”ä¾‹", 0.80, 0.99, 0.95, 0.01)

clf_name = st.sidebar.selectbox(
    "é€‰æ‹©åˆ†ç±»å™¨",
    ["KNN", "Linear SVM", "SVM (RBF)", "Logistic Regression", "Random Forest"]
)

# hyperparams
if clf_name == "KNN":
    knn_k = st.sidebar.slider("KNN: n_neighbors", 1, 25, 5, 1)
elif clf_name == "Linear SVM":
    linsvm_C = st.sidebar.slider("Linear SVM: C", 0.01, 10.0, 1.0, 0.01)
elif clf_name == "SVM (RBF)":
    rbf_C = st.sidebar.slider("SVM-RBF: C", 0.01, 10.0, 1.0, 0.01)
elif clf_name == "Logistic Regression":
    logreg_C = st.sidebar.slider("LogReg: C", 0.01, 10.0, 1.0, 0.01)
elif clf_name == "Random Forest":
    rf_n = st.sidebar.slider("RF: n_estimators", 50, 500, 300, 50)
    rf_depth = st.sidebar.slider("RF: max_depth (None=0)", 0, 50, 0, 1)

st.sidebar.divider()
st.sidebar.caption("ç¼ºå¤±å€¼å°†ä»¥åˆ—ä¸­ä½æ•°å¡«è¡¥ï¼›éæ ‘æ¨¡å‹ä¼šåšæ ‡å‡†åŒ–ã€‚ç±»åˆ«ä¸å¹³è¡¡æ—¶å¤§å¤šæ•°æ¨¡å‹ä¼šå¯ç”¨ class_weight='balanced'ï¼ˆKNNé™¤å¤–ï¼‰ã€‚")


# --- File upload ---
uploaded = st.file_uploader("ğŸ“¤ ä¸Šä¼  Excel æ–‡ä»¶ï¼ˆ.xlsxï¼‰", type=["xlsx"])
if uploaded is None:
    st.info("è¯·ä¸Šä¼  Excelã€‚ç¬¬ä¸€åˆ—å¿…é¡»æ˜¯**åŸå§‹æ ‡ç­¾**ï¼ˆæ•°å­—/å­—ç¬¦ä¸²éƒ½å¯ä»¥ï¼‰ï¼Œåé¢åˆ—æ˜¯æ•°å€¼ç‰¹å¾ã€‚")
    st.stop()

# Load excel
try:
    bytes_data = uploaded.read()
    xls = pd.ExcelFile(io.BytesIO(bytes_data))
    sheet_name = st.selectbox("é€‰æ‹©å·¥ä½œè¡¨", xls.sheet_names)
    df = pd.read_excel(io.BytesIO(bytes_data), sheet_name=sheet_name)
except Exception as e:
    st.error(f"è¯»å– Excel å¤±è´¥ï¼š{e}")
    st.stop()

if df.shape[1] < 2:
    st.error("è‡³å°‘éœ€è¦ä¸¤åˆ—ï¼šç¬¬ä¸€åˆ—ä¸ºåŸå§‹æ ‡ç­¾ï¼Œå…¶ä½™åˆ—ä¸ºç‰¹å¾ã€‚")
    st.stop()

st.subheader("æ•°æ®é¢„è§ˆ")
st.dataframe(df.head(), use_container_width=True)

# Identify columns
orig_label_col = df.columns[0]
feature_cols_all = [c for c in df.columns[1:]]

# Numeric casting for features; non-numeric will be coerced to NaN
df_features = df[feature_cols_all].apply(pd.to_numeric, errors="coerce")

# Show NaN report
with st.expander("ğŸ” ç¼ºå¤±å€¼æŠ¥å‘Š"):
    na_counts = df_features.isna().sum()
    st.write(na_counts.to_frame("NaNè®¡æ•°"))
    st.caption("ç‰¹å¾åˆ—ä¸­çš„éæ•°å€¼ã€ç©ºç™½ä¼šè¢«è½¬ä¸º NaNï¼›ç¨åä¼šä½¿ç”¨ä¸­ä½æ•°å¡«è¡¥ã€‚")

# --- Target class mapping ---
st.subheader("ğŸ¯ æ ‡ç­¾æ˜ å°„ä¸åˆ†ç±»è®¾ç½®")
unique_orig_labels = pd.Series(df[orig_label_col].unique()).tolist()
unique_orig_labels_sorted = sorted(unique_orig_labels, key=lambda x: str(x))

num_classes = st.radio("ç›®æ ‡åˆ†ç±»æ•°", [2, 3, 4, 5], horizontal=True)

st.write("è¯·ä¸ºæ¯ä¸ª**åŸå§‹æ ‡ç­¾å€¼**æŒ‡å®šä¸€ä¸ªæ–°çš„**ç›®æ ‡ç±»åˆ«**ç¼–å·ï¼ˆ0..K-1ï¼‰ï¼š")
mapping: Dict = {}
cols = st.columns(2)
for idx, val in enumerate(unique_orig_labels_sorted):
    with cols[idx % 2]:
        new_lab = st.selectbox(
            f"åŸå§‹æ ‡ç­¾ {val} â†’",
            options=list(range(num_classes)),
            key=f"map_{idx}",
            index=0
        )
        mapping[val] = new_lab

st.caption(f"å½“å‰æ˜ å°„ï¼š{mapping}")

# Build y_new using mapping
try:
    y_new = df[orig_label_col].map(mapping)
except Exception as e:
    st.error(f"æ ‡ç­¾æ˜ å°„å¤±è´¥ï¼š{e}")
    st.stop()

# Drop rows where mapping failed (shouldn't happen if mapping is defined for all)
mask_valid = y_new.notna()
dropped = (~mask_valid).sum()
if dropped > 0:
    st.warning(f"æœ‰ {dropped} è¡Œæ ‡ç­¾æœªæˆåŠŸæ˜ å°„ï¼Œå·²ç§»é™¤ã€‚è¯·æ£€æŸ¥æ˜ å°„æ˜¯å¦è¦†ç›–å…¨éƒ¨åŸå§‹æ ‡ç­¾ã€‚")
y = y_new[mask_valid].astype(int).values
X_all = df_features.loc[mask_valid].copy()

# Feature selection
st.subheader("ğŸ§© ç‰¹å¾é€‰æ‹©")
default_feats = feature_cols_all  # é»˜è®¤å…¨é€‰
chosen_feats = st.multiselect(
    "é€‰æ‹©ç”¨äºè®­ç»ƒçš„ç‰¹å¾åˆ—ï¼ˆå¯å¤šé€‰ï¼‰",
    options=feature_cols_all,
    default=default_feats
)

if len(chosen_feats) == 0:
    st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾ã€‚")
    st.stop()

X = X_all[chosen_feats].values

# Train/test split (stratified)
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
except ValueError as e:
    st.error(f"åˆ’åˆ†æ•°æ®å¤±è´¥ï¼š{e}ï¼ˆå¯èƒ½æŸä¸€ç±»æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•åˆ†å±‚åˆ’åˆ†ï¼‰")
    st.stop()

# Build model pipeline
num_classes_int = int(num_classes)
average_mode = "binary" if num_classes_int == 2 else "macro"

def make_non_tree_pipeline(base_estimator):
    steps = [
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
    ]
    if use_pca and clf_name != "Random Forest":
        steps.append(("pca", PCA(n_components=pca_var, svd_solver="full")))
    steps.append(("clf", base_estimator))
    return Pipeline(steps)

def make_tree_pipeline(base_estimator):
    # æ ‘æ¨¡å‹ä¸éœ€è¦æ ‡å‡†åŒ–ï¼›ä½†ä¿ç•™å¡«è¡¥
    return Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("clf", base_estimator)
    ])

# Instantiate classifier
if clf_name == "KNN":
    clf = make_non_tree_pipeline(KNeighborsClassifier(n_neighbors=knn_k))
elif clf_name == "Linear SVM":
    # LinearSVC ä¸æ”¯æŒ predict_probaï¼›class_weight å¹³è¡¡
    clf = make_non_tree_pipeline(LinearSVC(C=linsvm_C, class_weight="balanced", dual="auto"))
elif clf_name == "SVM (RBF)":
    clf = make_non_tree_pipeline(SVC(C=rbf_C, kernel="rbf", class_weight="balanced", probability=True))
elif clf_name == "Logistic Regression":
    clf = make_non_tree_pipeline(LogisticRegression(C=logreg_C, max_iter=2000, class_weight="balanced"))
elif clf_name == "Random Forest":
    md = None if rf_depth == 0 else rf_depth
    clf = make_tree_pipeline(RandomForestClassifier(
        n_estimators=rf_n,
        max_depth=md,
        random_state=random_state,
        class_weight="balanced"
    ))
else:
    st.error("æœªçŸ¥çš„åˆ†ç±»å™¨")
    st.stop()

# Train
st.subheader("ğŸš€ è®­ç»ƒä¸è¯„ä¼°")
if st.button("è®­ç»ƒå¹¶è¯„ä¼°", type="primary"):
    try:
        clf.fit(X_train, y_train)
    except Exception as e:
        st.error(f"è®­ç»ƒå¤±è´¥ï¼š{e}")
        st.stop()

    # Predict
    y_pred = clf.predict(X_test)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average=average_mode)

    left, right, right2 = st.columns(3)
    with left:
        st.metric("Accuracy", f"{acc:.4f}")
    with right:
        st.metric(f"F1 ({average_mode})", f"{f1:.4f}")

    # ROC AUC only for binary if proba/decision available
    auc_shown = False
    if num_classes_int == 2:
        try:
            # decision_function preferred; else predict_proba
            if hasattr(clf, "decision_function"):
                y_score = clf.decision_function(X_test)
            elif hasattr(clf, "predict_proba"):
                y_score = clf.predict_proba(X_test)[:, 1]
            else:
                y_score = None
            if y_score is not None:
                auc = roc_auc_score(y_test, y_score)
                with right2:
                    st.metric("ROC AUC", f"{auc:.4f}")
                auc_shown = True
        except Exception:
            pass
    if num_classes_int != 2:
        st.caption("å¤šåˆ†ç±»æš‚ä¸è®¡ç®— AUCï¼ˆå¯æ‰©å±•æˆ OVO/OVRï¼‰ã€‚")

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred, labels=list(range(num_classes_int)))
    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation="nearest")
    ax.figure.colorbar(im, ax=ax)
    ax.set(
        xticks=np.arange(num_classes_int),
        yticks=np.arange(num_classes_int),
        xticklabels=[f"Pred {i}" for i in range(num_classes_int)],
        yticklabels=[f"True {i}" for i in range(num_classes_int)],
        ylabel="True label",
        xlabel="Predicted label",
        title="Confusion Matrix"
    )
    # write numbers
    for i in range(num_classes_int):
        for j in range(num_classes_int):
            ax.text(j, i, cm[i, j], ha="center", va="center")
    st.pyplot(fig)

    # Classification report (text)
    st.subheader("ğŸ“„ åˆ†ç±»æŠ¥å‘Š")
    cr = classification_report(y_test, y_pred, digits=4, zero_division=0)
    st.code(cr, language="text")

    # Feature importance / coefficients
    st.subheader("ğŸ§­ å¯è§£é‡Šæ€§")
    try:
        if clf_name == "Random Forest":
            # Extract inner estimator
            final = clf.named_steps["clf"]
            importances = final.feature_importances_
            # If PCA applied (we disabled for RF), skip mapping
            imp_df = pd.DataFrame({"feature": chosen_feats, "importance": importances}).sort_values("importance", ascending=False)
            st.dataframe(imp_df, use_container_width=True)
            fig2, ax2 = plt.subplots(figsize=(6, min(0.35*len(chosen_feats)+1, 10)))
            ax2.barh(imp_df["feature"][::-1], imp_df["importance"][::-1])
            ax2.set_title("Feature Importance (RF)")
            st.pyplot(fig2)

        elif clf_name in ["Linear SVM", "Logistic Regression"]:
            # pipeline may include scaler (and maybe PCA). å¦‚æœç”¨äº† PCAï¼Œç³»æ•°åœ¨é™ç»´ç©ºé—´ï¼Œè§£é‡Šæ€§å¼±
            final = clf.named_steps["clf"]
            if hasattr(final, "coef_"):
                coef = final.coef_
                if num_classes_int == 2:
                    coef = coef[0]
                    names = chosen_feats
                    if use_pca:
                        st.warning("ä½¿ç”¨äº† PCAï¼šçº¿æ€§æ¨¡å‹ç³»æ•°åœ¨é™ç»´åçš„ç‰¹å¾ç©ºé—´ï¼Œç›´æ¥å¯¹åº”åŸå§‹ç‰¹å¾çš„è§£é‡Šæ€§è¾ƒå¼±ã€‚")
                    coef_df = pd.DataFrame({"feature": names, "coef": coef if len(coef)==len(names) else np.nan})
                    coef_df = coef_df.sort_values("coef", key=np.abs, ascending=False)
                    st.dataframe(coef_df, use_container_width=True)
                    fig3, ax3 = plt.subplots(figsize=(6, min(0.35*len(chosen_feats)+1, 10)))
                    ax3.barh(coef_df["feature"][::-1], coef_df["coef"][::-1])
                    ax3.set_title("Linear Model Coefficients")
                    st.pyplot(fig3)
                else:
                    st.caption("å¤šåˆ†ç±»çº¿æ€§ç³»æ•°ä¸º one-vs-rest çŸ©é˜µï¼Œæš‚ä¸ç»˜åˆ¶ï¼Œå¯åœ¨æ•°æ®æ¡†ä¸­æŸ¥çœ‹ã€‚")
                    st.dataframe(pd.DataFrame(coef, columns=chosen_feats), use_container_width=True)
            else:
                st.caption("è¯¥æ¨¡å‹æ²¡æœ‰å¯ç”¨çš„çº¿æ€§ç³»æ•°ã€‚")
        else:
            st.caption("KNN / SVM-RBF æ²¡æœ‰ç›´æ¥çš„ç‰¹å¾é‡è¦æ€§ã€‚")

    except Exception as e:
        st.warning(f"å¯è§£é‡Šæ€§è®¡ç®—é‡åˆ°é—®é¢˜ï¼š{e}")

st.divider()
st.caption("å°è´´å£«ï¼šå¦‚æœæ ·æœ¬å¤ªå°‘å¯¼è‡´æŸç±»åœ¨æµ‹è¯•é›†ä¸­ä¸º 0ï¼Œè¯•ç€å‡å°æµ‹è¯•é›†æ¯”ä¾‹æˆ–åˆå¹¶ç±»åˆ«ã€‚")
